{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decision Tree Implementation with \"Pseudo\" Feature Embedding\n",
    "#Huy Dinh Tran\n",
    "#Reference: https://machinelearningmastery.com/implement-decision-tree-algorithm-scratch-python/ \n",
    "\n",
    "#Importing dependencies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import tree\n",
    "from IPython.display import Image, display\n",
    "import matplotlib.pyplot as plt\n",
    "import pydotplus\n",
    "import matplotlib.image as pltimg\n",
    "from dtreeviz.trees import dtreeviz\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree Classification Model from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Node Class\n",
    "#Each node holds feature index, threshold, left and right output, and info gain value\n",
    "class Node():\n",
    "    def __init__(self, feature_index=None, threshold=None, left=None, right=None, info_gain=None, value=None):\n",
    "        # for decision node\n",
    "        self.feature_index = feature_index\n",
    "        self.threshold = threshold\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.info_gain = info_gain\n",
    "        \n",
    "        # for leaf node\n",
    "        self.value = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tree Class\n",
    "class DecisionTreeClassifier():\n",
    "\n",
    "    #initializing the tree\n",
    "    def __init__(self, min_samples_split, max_depth): \n",
    "        self.root = None\n",
    "        \n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.max_depth = max_depth\n",
    "    \n",
    "    #Recursion function using other functions to build an optimal tree\n",
    "    def build_tree(self, dataset, curr_depth=0):        \n",
    "        X, Y = dataset[:,:-1], dataset[:,-1]\n",
    "        num_samples, num_features = np.shape(X)\n",
    "        \n",
    "        # split until stopping conditions are met\n",
    "        if num_samples >= self.min_samples_split and curr_depth <= self.max_depth:\n",
    "            # find the best split\n",
    "            best_split = self.get_best_split(dataset, num_samples, num_features)\n",
    "            # check if information gain is positive\n",
    "            if best_split[\"info_gain\"] > 0:\n",
    "                # recur left\n",
    "                left_subtree = self.build_tree(best_split[\"dataset_left\"], curr_depth+1)\n",
    "                # recur right\n",
    "                right_subtree = self.build_tree(best_split[\"dataset_right\"], curr_depth+1)\n",
    "                # return decision node\n",
    "                return Node(best_split[\"feature_index\"], best_split[\"threshold\"], \n",
    "                            left_subtree, right_subtree, best_split[\"info_gain\"])\n",
    "        \n",
    "        # compute leaf node\n",
    "        leaf_value = self.calculate_leaf_value(Y)\n",
    "        # return leaf node\n",
    "        return Node(value=leaf_value)\n",
    "    \n",
    "    #finding the best split after trying different configurations\n",
    "    def get_best_split(self, dataset, num_samples, num_features):        \n",
    "        # dictionary to store the best split\n",
    "        best_split = {}\n",
    "        max_info_gain = -float(\"inf\")\n",
    "        \n",
    "        # loop over all the features\n",
    "        for feature_index in range(num_features):\n",
    "            feature_values = dataset[:, feature_index]\n",
    "            possible_thresholds = np.unique(feature_values)\n",
    "            # loop over all the feature values present in the data\n",
    "            for threshold in possible_thresholds:\n",
    "                # get current split\n",
    "                dataset_left, dataset_right = self.split(dataset, feature_index, threshold)\n",
    "                # check if childs are not null\n",
    "                if len(dataset_left) > 0 and len(dataset_right) > 0:\n",
    "                    y, left_y, right_y = dataset[:, -1], dataset_left[:, -1], dataset_right[:, -1]\n",
    "                    # compute information gain\n",
    "                    curr_info_gain = self.information_gain(y, left_y, right_y, \"gini\")\n",
    "                    # update the best split if needed\n",
    "                    if curr_info_gain > max_info_gain:\n",
    "                        best_split[\"feature_index\"] = feature_index\n",
    "                        best_split[\"threshold\"] = threshold\n",
    "                        best_split[\"dataset_left\"] = dataset_left\n",
    "                        best_split[\"dataset_right\"] = dataset_right\n",
    "                        best_split[\"info_gain\"] = curr_info_gain\n",
    "                        max_info_gain = curr_info_gain\n",
    "                        \n",
    "        # return best split\n",
    "        return best_split\n",
    "    \n",
    "    #spliting the data left and right\n",
    "    def split(self, dataset, feature_index, threshold):\n",
    "        dataset_left = np.array([row for row in dataset if row[feature_index]<=threshold])\n",
    "        dataset_right = np.array([row for row in dataset if row[feature_index]>threshold])\n",
    "        return dataset_left, dataset_right\n",
    "    \n",
    "    #output info gain value\n",
    "    def information_gain(self, parent, l_child, r_child, mode=\"entropy\"):        \n",
    "        weight_l = len(l_child) / len(parent)\n",
    "        weight_r = len(r_child) / len(parent)\n",
    "        if mode == \"gini\":\n",
    "            gain = self.gini(parent) - (weight_l*self.gini(l_child) + weight_r*self.gini(r_child))\n",
    "        else:\n",
    "            gain = self.entropy(parent) - (weight_l*self.entropy(l_child) + weight_r*self.entropy(r_child))\n",
    "        return gain\n",
    "    \n",
    "    #output entropy value\n",
    "    def entropy(self, y):\n",
    "        class_labels = np.unique(y)\n",
    "        entropy = 0\n",
    "        for cls in class_labels:\n",
    "            p_cls = len(y[y == cls]) / len(y)\n",
    "            entropy += -p_cls * np.log2(p_cls)\n",
    "        return entropy\n",
    "    \n",
    "    #output gini value\n",
    "    def gini(self, y):        \n",
    "        class_labels = np.unique(y)\n",
    "        gini = 0\n",
    "        for cls in class_labels:\n",
    "            p_cls = len(y[y == cls]) / len(y)\n",
    "            gini += p_cls**2\n",
    "        return 1 - gini\n",
    "        \n",
    "    #output leaf node\n",
    "    def calculate_leaf_value(self, Y):\n",
    "        Y = list(Y)\n",
    "        return max(Y, key=Y.count)\n",
    "    \n",
    "    #print dicision tree\n",
    "    def print_tree(self, tree=None, indent=\" \"):\n",
    "        if not tree:\n",
    "            tree = self.root\n",
    "\n",
    "        if tree.value is not None:\n",
    "            print(tree.value)\n",
    "\n",
    "        else:\n",
    "            print(\"X_\"+str(tree.feature_index), \"<=\", tree.threshold, \"?\", tree.info_gain)\n",
    "            print(\"%sleft:\" % (indent), end=\"\")\n",
    "            self.print_tree(tree.left, indent + indent)\n",
    "            print(\"%sright:\" % (indent), end=\"\")\n",
    "            self.print_tree(tree.right, indent + indent)\n",
    "    \n",
    "    #train decision tree model\n",
    "    def fit(self, X, Y):\n",
    "        dataset = np.concatenate((X, Y), axis=1)\n",
    "        self.root = self.build_tree(dataset)\n",
    "    \n",
    "    #test/predict new dataset\n",
    "    def predict(self, X):        \n",
    "        preditions = [self.one_prediction(x, self.root) for x in X]\n",
    "        return preditions\n",
    "\n",
    "    #predict only one data point\n",
    "    def one_prediction(self, x, tree):        \n",
    "        if tree.value != None: \n",
    "            return tree.value\n",
    "        feature_val = x[tree.feature_index]\n",
    "        if feature_val <= tree.threshold:\n",
    "            return self.one_prediction(x, tree.left)\n",
    "        else:\n",
    "            return self.one_prediction(x, tree.right)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading and spliting our dataset to training and testing data\n",
    "trainingData = pd.read_csv(\"../../Data/Dataset.csv\")\n",
    "testingData = pd.read_csv(\"../../Data/TestingData.csv\").values\n",
    "\n",
    "targets = trainingData[\"home_team_result\"]\n",
    "features = trainingData.drop([\"home_team_result\",\"winner_encoded\"],axis=1).values\n",
    "\n",
    "featuresTrain, targetsTrain = features[0:48], targets[0:48]\n",
    "featuresTest, targetsTest = testingData, targets[48:]\n",
    "targetsTrain=targetsTrain.values.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  50.0 %\n"
     ]
    }
   ],
   "source": [
    "#Training\n",
    "dtree = DecisionTreeClassifier(min_samples_split=20, max_depth=20)\n",
    "dtree.fit(featuresTrain, targetsTrain)\n",
    "\n",
    "#Testing\n",
    "test = dtree.predict(featuresTest) \n",
    "print('Accuracy: ',accuracy_score(targetsTest, test)*100,'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_2 <= 1.0 ? 0.2556991882774616\n",
      " left:X_3 <= 0.0 ? 0.2501400784439287\n",
      "  left:1.0\n",
      "  right:X_8 <= 3.0 ? 0.0355029585798817\n",
      "    left:0.0\n",
      "    right:0.0\n",
      " right:1.0\n"
     ]
    }
   ],
   "source": [
    "#print decision tree\n",
    "dtree.print_tree()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "50268d6ed2a5ae339fbbb8319d22bf0851ab34d7c7cc113d9fe8d697a6e9c413"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
