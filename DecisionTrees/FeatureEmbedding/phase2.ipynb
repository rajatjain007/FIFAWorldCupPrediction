{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decision Tree Implementation with \"Pseudo\" Feature Embedding\n",
    "#Reference: https://machinelearningmastery.com/implement-decision-tree-algorithm-scratch-python/ \n",
    "\n",
    "#Importing dependencies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree Classification Model from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Node Class\n",
    "#Each node holds feature index, threshold, left and right output, and info gain value\n",
    "class Node():\n",
    "    def __init__(self, featureIndex=None, threshold=None, left=None, right=None, infoGain=None, value=None):\n",
    "        #for decision node\n",
    "        self.featureIndex = featureIndex\n",
    "        self.threshold = threshold\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.infoGain = infoGain\n",
    "        \n",
    "        #for leaf node\n",
    "        self.value = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tree Class\n",
    "#Include functions that help configure and build a decision tree\n",
    "class DecisionTreeClassifier():\n",
    "\n",
    "    #initializing the tree\n",
    "    def __init__(self, minSamplesSplit, maxDepth): \n",
    "        self.root = None\n",
    "        \n",
    "        self.minSamplesSplit = minSamplesSplit\n",
    "        self.maxDepth = maxDepth\n",
    "    \n",
    "    #Recursion function using other functions to build an optimal tree\n",
    "    def buildTree(self, dataset, currDepth=0):        \n",
    "        X, Y = dataset[:,:-1], dataset[:,-1]\n",
    "        numSamples, numFeatures = np.shape(X)\n",
    "        \n",
    "        if numSamples >= self.minSamplesSplit and currDepth <= self.maxDepth:\n",
    "            bestSplit = self.getBestSplit(dataset, numFeatures)\n",
    "            if bestSplit[\"infoGain\"] > 0:\n",
    "                leftSubtree = self.buildTree(bestSplit[\"datasetLeft\"], currDepth+1)\n",
    "                rightSubtree = self.buildTree(bestSplit[\"datasetRight\"], currDepth+1)\n",
    "                return Node(bestSplit[\"featureIndex\"], bestSplit[\"threshold\"], leftSubtree, rightSubtree, bestSplit[\"infoGain\"])\n",
    "        \n",
    "        leafValue = self.calculateLeafValue(Y)\n",
    "        return Node(value=leafValue)\n",
    "    \n",
    "    #finding the best split after trying different configurations\n",
    "    def getBestSplit(self, dataset, numFeatures):        \n",
    "        bestSplit = {}\n",
    "        maxInfoGain = -float(\"inf\")\n",
    "        \n",
    "        for featureIndex in range(numFeatures):\n",
    "            featureValues = dataset[:, featureIndex]\n",
    "            possibleThresholds = np.unique(featureValues)\n",
    "\n",
    "            for threshold in possibleThresholds:\n",
    "                datasetLeft, datasetRight = self.split(dataset, featureIndex, threshold)\n",
    "\n",
    "                if len(datasetLeft) > 0 and len(datasetRight) > 0:\n",
    "                    y, leftY, rightY = dataset[:, -1], datasetLeft[:, -1], datasetRight[:, -1]\n",
    "                    currInfoGain = self.informationGain(y, leftY, rightY, \"gini\")\n",
    "                    \n",
    "                    if currInfoGain > maxInfoGain:\n",
    "                        bestSplit[\"featureIndex\"] = featureIndex\n",
    "                        bestSplit[\"threshold\"] = threshold\n",
    "                        bestSplit[\"datasetLeft\"] = datasetLeft\n",
    "                        bestSplit[\"datasetRight\"] = datasetRight\n",
    "                        bestSplit[\"infoGain\"] = currInfoGain\n",
    "                        maxInfoGain = currInfoGain\n",
    "                        \n",
    "        return bestSplit\n",
    "    \n",
    "    #spliting the data left and right\n",
    "    def split(self, dataset, featureIndex, threshold):\n",
    "        datasetLeft = np.array([row for row in dataset if row[featureIndex]<=threshold])\n",
    "        datasetRight = np.array([row for row in dataset if row[featureIndex]>threshold])\n",
    "        return datasetLeft, datasetRight\n",
    "    \n",
    "    #output info gain value\n",
    "    def informationGain(self, parent, leftChild, rightChild, mode=\"entropy\"):        \n",
    "        weightLeft = len(leftChild) / len(parent)\n",
    "        weightRight = len(rightChild) / len(parent)\n",
    "        if mode == \"gini\":\n",
    "            gain = self.gini(parent) - (weightLeft*self.gini(leftChild) + weightRight*self.gini(rightChild))\n",
    "        else:\n",
    "            gain = self.entropy(parent) - (weightLeft*self.entropy(leftChild) + weightRight*self.entropy(rightChild))\n",
    "        return gain\n",
    "    \n",
    "    #output entropy value\n",
    "    def entropy(self, y):\n",
    "        classLabels = np.unique(y)\n",
    "        entropy = 0\n",
    "        for cls in classLabels:\n",
    "            pCls = len(y[y == cls]) / len(y)\n",
    "            entropy += -pCls * np.log2(pCls)\n",
    "        return entropy\n",
    "    \n",
    "    #output gini value\n",
    "    def gini(self, y):        \n",
    "        classLabels = np.unique(y)\n",
    "        gini = 0\n",
    "        for cls in classLabels:\n",
    "            pCls = len(y[y == cls]) / len(y)\n",
    "            gini += pCls**2\n",
    "        return 1 - gini\n",
    "        \n",
    "    #output leaf node\n",
    "    def calculateLeafValue(self, Y):\n",
    "        Y = list(Y)\n",
    "        return max(Y, key=Y.count)\n",
    "    \n",
    "    #print dicision tree\n",
    "    def printTree(self, tree=None, indent=\" \"):\n",
    "        if not tree:\n",
    "            tree = self.root\n",
    "\n",
    "        if tree.value is not None:\n",
    "            print(tree.value)\n",
    "\n",
    "        else:\n",
    "            print(\"X\"+str(tree.featureIndex), \"<=\", tree.threshold, \"?\", tree.infoGain)\n",
    "            print(\"%sLeft:\" % (indent), end=\"\")\n",
    "            self.printTree(tree.left, indent + indent)\n",
    "            print(\"%sRight:\" % (indent), end=\"\")\n",
    "            self.printTree(tree.right, indent + indent)\n",
    "    \n",
    "    #train decision tree model\n",
    "    def fit(self, X, Y):\n",
    "        dataset = np.concatenate((X, Y), axis=1)\n",
    "        self.root = self.buildTree(dataset)\n",
    "    \n",
    "    #test/predict new dataset\n",
    "    def predict(self, X):        \n",
    "        preditions = [self.onePrediction(x, self.root) for x in X]\n",
    "        return preditions\n",
    "\n",
    "    #predict only one data point\n",
    "    def onePrediction(self, x, tree):        \n",
    "        if tree.value != None: \n",
    "            return tree.value\n",
    "        featureVal = x[tree.featureIndex]\n",
    "        if featureVal <= tree.threshold:\n",
    "            return self.onePrediction(x, tree.left)\n",
    "        else:\n",
    "            return self.onePrediction(x, tree.right)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading and spliting our dataset to training and testing data\n",
    "trainingData = pd.read_csv(\"../../Data/Dataset.csv\")\n",
    "testingData = pd.read_csv(\"../../Data/TestingData.csv\").values\n",
    "\n",
    "targets = trainingData[\"home_team_result\"]\n",
    "features = trainingData.drop([\"home_team_result\",\"winner_encoded\"],axis=1).values\n",
    "\n",
    "featuresTrain, targetsTrain = features[0:48], targets[0:48]\n",
    "featuresTest, targetsTest = testingData, targets[48:]\n",
    "targetsTrain=targetsTrain.values.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  50.0 %\n"
     ]
    }
   ],
   "source": [
    "#Training\n",
    "dtree = DecisionTreeClassifier(20, 20)\n",
    "dtree.fit(featuresTrain, targetsTrain)\n",
    "\n",
    "#Testing\n",
    "test = dtree.predict(featuresTest) \n",
    "print('Accuracy: ',accuracy_score(targetsTest, test)*100,'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X2 <= 1.0 ? 0.2556991882774616\n",
      " Left:X3 <= 0.0 ? 0.2501400784439287\n",
      "  Left:1.0\n",
      "  Right:X8 <= 3.0 ? 0.0355029585798817\n",
      "    Left:0.0\n",
      "    Right:0.0\n",
      " Right:1.0\n"
     ]
    }
   ],
   "source": [
    "#print decision tree\n",
    "dtree.printTree()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b80aff2a19cf941dbb44256e5b2c5db4bea0d59041b979103d19485f1a4c9597"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
