{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decision Tree Implementation with \"Pseudo\" Feature Embedding\n",
    "#Reference: https://machinelearningmastery.com/implement-decision-tree-algorithm-scratch-python/ \n",
    "\n",
    "#Importing dependencies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree Classification Model from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Node Class\n",
    "#Each node holds feature index, threshold, left and right output, and info gain value\n",
    "class Node():\n",
    "    def __init__(self, featureIndex=None, threshold=None, left=None, right=None, infoGain=None, value=None):\n",
    "        #for decision node\n",
    "        self.featureIndex = featureIndex\n",
    "        self.threshold = threshold\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.infoGain = infoGain\n",
    "        \n",
    "        #for leaf node\n",
    "        self.value = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tree Class\n",
    "#Include functions that help configure and build a decision tree\n",
    "class DecisionTreeClassifier():\n",
    "\n",
    "    #initializing the tree\n",
    "    def __init__(self,minSamplesSplit,maxDepth): \n",
    "        self.root = None\n",
    "        self.minSamplesSplit = minSamplesSplit\n",
    "        self.maxDepth = maxDepth\n",
    "    \n",
    "    #predict only one data point\n",
    "    def onePrediction(self,x,tree):        \n",
    "        if tree.value != None: \n",
    "\n",
    "            return tree.value\n",
    "        featureVal = x[tree.featureIndex]\n",
    "\n",
    "        if featureVal <= tree.threshold:\n",
    "\n",
    "            return self.onePrediction(x,tree.left)\n",
    "\n",
    "        else:\n",
    "            \n",
    "            return self.onePrediction(x,tree.right)\n",
    "\n",
    "    #test/predict new dataset\n",
    "    def predict(self,X):        \n",
    "        preditions = [self.onePrediction(x,self.root) for x in X]\n",
    "        return preditions\n",
    "\n",
    "    #output leaf node\n",
    "    def calculateLeafValue(self,Y):\n",
    "        Y = list(Y)\n",
    "        return max(Y, key=Y.count)\n",
    "    \n",
    "    #output gini value\n",
    "    def getGini(self,y):        \n",
    "        classLabels = np.unique(y)\n",
    "        gini = 0\n",
    "\n",
    "        for cls in classLabels:\n",
    "            pCls = len(y[y == cls]) / len(y)\n",
    "            gini += pCls ** 2\n",
    "\n",
    "        return 1 - gini\n",
    "    \n",
    "    #output entropy value\n",
    "    def getEntropy(self,y):\n",
    "        classLabels = np.unique(y)\n",
    "        entropy = 0\n",
    "\n",
    "        for cls in classLabels:\n",
    "            pCls = len(y[y == cls]) / len(y)\n",
    "            entropy += -pCls * np.log2(pCls)\n",
    "\n",
    "        return entropy\n",
    "\n",
    "    #output info gain value\n",
    "    def getInformationGain(self,parent,leftChild,rightChild,mode=\"entropy\"):        \n",
    "        weightLeft = len(leftChild) / len(parent)\n",
    "        weightRight = len(rightChild) / len(parent)\n",
    "\n",
    "        if mode == \"gini\":\n",
    "            gain = self.getGini(parent) - (weightLeft * self.getGini(leftChild) + weightRight * self.getGini(rightChild))\n",
    "        \n",
    "        else:\n",
    "            gain = self.getEntropy(parent) - (weightLeft * self.getEntropy(leftChild) + weightRight * self.getEntropy(rightChild))\n",
    "\n",
    "        return gain\n",
    "\n",
    "    #spliting the data left and right\n",
    "    def split(self,dataset,featureIndex,threshold):\n",
    "        datasetLeft = np.array([row for row in dataset if row[featureIndex] <= threshold])\n",
    "        datasetRight = np.array([row for row in dataset if row[featureIndex] > threshold])\n",
    "\n",
    "        return datasetLeft, datasetRight        \n",
    "\n",
    "    #finding the best split after trying different features and thresholds\n",
    "    def getBestSplit(self,dataset,numFeatures):        \n",
    "        bestSplit = {}\n",
    "        maxInfoGain = -float(\"inf\")\n",
    "        \n",
    "        for featureIndex in range(numFeatures):\n",
    "            featureValues = dataset[:,featureIndex]\n",
    "            possibleThresholds = np.unique(featureValues)\n",
    "\n",
    "            for threshold in possibleThresholds:\n",
    "                datasetLeft, datasetRight = self.split(dataset,featureIndex,threshold)\n",
    "\n",
    "                if len(datasetLeft) > 0 and len(datasetRight) > 0:\n",
    "                    y, leftY, rightY = dataset[:,-1], datasetLeft[:, -1], datasetRight[:,-1]\n",
    "                    currInfoGain = self.getInformationGain(y,leftY,rightY,\"gini\")\n",
    "                    \n",
    "                    if currInfoGain > maxInfoGain:\n",
    "                        bestSplit[\"featureIndex\"] = featureIndex\n",
    "                        bestSplit[\"threshold\"] = threshold\n",
    "                        bestSplit[\"datasetLeft\"] = datasetLeft\n",
    "                        bestSplit[\"datasetRight\"] = datasetRight\n",
    "                        bestSplit[\"infoGain\"] = currInfoGain\n",
    "                        maxInfoGain = currInfoGain\n",
    "                        \n",
    "        return bestSplit\n",
    "\n",
    "    #Recursion function using other functions to build an optimal tree\n",
    "    def buildTree(self, dataset, currDepth=0):        \n",
    "        X, Y = dataset[:,:-1], dataset[:,-1]\n",
    "        numSamples, numFeatures = np.shape(X)\n",
    "        \n",
    "        if numSamples >= self.minSamplesSplit and currDepth <= self.maxDepth:\n",
    "            bestSplit = self.getBestSplit(dataset, numFeatures)\n",
    "\n",
    "            if bestSplit[\"infoGain\"] > 0:\n",
    "                leftSubtree = self.buildTree(bestSplit[\"datasetLeft\"], currDepth + 1)\n",
    "                rightSubtree = self.buildTree(bestSplit[\"datasetRight\"], currDepth + 1)\n",
    "\n",
    "                return Node(bestSplit[\"featureIndex\"], bestSplit[\"threshold\"], leftSubtree, rightSubtree, bestSplit[\"infoGain\"])\n",
    "        \n",
    "        leafValue = self.calculateLeafValue(Y)\n",
    "\n",
    "        return Node(value=leafValue)\n",
    "\n",
    "    #train decision tree model\n",
    "    def fit(self,X,Y):\n",
    "        dataset = np.concatenate((X, Y), axis=1)\n",
    "        self.root = self.buildTree(dataset)\n",
    "\n",
    "    #print dicision tree\n",
    "    def printTree(self, tree=None, indent=\" \"):\n",
    "        if not tree:\n",
    "            tree = self.root\n",
    "\n",
    "        if tree.value is not None:\n",
    "            print(tree.value)\n",
    "\n",
    "        else:\n",
    "            print(\"featureIndex X\"+str(tree.featureIndex), \"<=\", tree.threshold, \"? infoGain:\", tree.infoGain)\n",
    "            print(\"%sLeft:\" % (indent), end=\"\")\n",
    "            self.printTree(tree.left, indent + indent)\n",
    "            print(\"%sRight:\" % (indent), end=\"\")\n",
    "            self.printTree(tree.right, indent + indent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading and spliting our dataset to training and testing data\n",
    "trainingData = pd.read_csv(\"../Data/LargeTrainingData.csv\")\n",
    "testingData = pd.read_csv(\"../Data/LargeTestingData.csv\")\n",
    "\n",
    "# Training Data \n",
    "targets = trainingData[\"home_team_result\"]\n",
    "targets = targets.values.reshape(-1,1)\n",
    "features = trainingData.drop([\"home_team_name\", \"away_team_name\", \"home_team_goal_count\", \"away_team_goal_count\", \"home_team_result\",\"winner_encoded\"],axis=1).values\n",
    "featuresTrain = features\n",
    "targetsTrain = targets\n",
    "\n",
    "# Testing Data\n",
    "targets = testingData[\"home_team_result\"]\n",
    "targets = targets.values.reshape(-1,1)\n",
    "features = testingData.drop([\"home_team_name\", \"away_team_name\", \"home_team_goal_count\", \"away_team_goal_count\", \"home_team_result\"],axis=1).values\n",
    "featuresTest = features\n",
    "targetsTest = targets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  56.25 %\n"
     ]
    }
   ],
   "source": [
    "#Training\n",
    "dtree = DecisionTreeClassifier(6, 6)\n",
    "dtree.fit(featuresTrain, targetsTrain)\n",
    "\n",
    "#Testing\n",
    "test = dtree.predict(featuresTest) \n",
    "print('Accuracy: ',accuracy_score(targetsTest, test)*100,'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "featureIndex X8 <= 10.0 ? infoGain: 0.03023583954847825\n",
      " Left:featureIndex X9 <= 2.0 ? infoGain: 0.028523769988264225\n",
      "  Left:featureIndex X6 <= 7.0 ? infoGain: 0.16685677040899483\n",
      "    Left:0.0\n",
      "    Right:featureIndex X0 <= 12.0 ? infoGain: 0.11072664359861584\n",
      "        Left:1.0\n",
      "        Right:0.0\n",
      "  Right:featureIndex X10 <= 7.0 ? infoGain: 0.0349105585966466\n",
      "    Left:featureIndex X13 <= 12.0 ? infoGain: 0.025495252347104147\n",
      "        Left:featureIndex X14 <= 33.0 ? infoGain: 0.061224489795918435\n",
      "                Left:featureIndex X1 <= 9.0 ? infoGain: 0.2777777777777777\n",
      "                                Left:1.0\n",
      "                                Right:0.0\n",
      "                Right:featureIndex X14 <= 57.0 ? infoGain: 0.08080808080808077\n",
      "                                Left:featureIndex X4 <= 0.0 ? infoGain: 0.04099435917617733\n",
      "                                                                Left:0.0\n",
      "                                                                Right:0.0\n",
      "                                Right:1.0\n",
      "        Right:featureIndex X1 <= 2.0 ? infoGain: 0.04081216202428328\n",
      "                Left:featureIndex X8 <= 5.0 ? infoGain: 0.2962962962962963\n",
      "                                Left:featureIndex X10 <= 2.0 ? infoGain: 0.19753086419753085\n",
      "                                                                Left:1.0\n",
      "                                                                Right:0.0\n",
      "                                Right:1.0\n",
      "                Right:featureIndex X14 <= 42.0 ? infoGain: 0.031761775086495025\n",
      "                                Left:featureIndex X13 <= 14.0 ? infoGain: 0.19889807162534462\n",
      "                                                                Left:1.0\n",
      "                                                                Right:0.0\n",
      "                                Right:featureIndex X10 <= 6.0 ? infoGain: 0.02103238988041589\n",
      "                                                                Left:1.0\n",
      "                                                                Right:1.0\n",
      "    Right:featureIndex X1 <= 4.0 ? infoGain: 0.06807003563760305\n",
      "        Left:featureIndex X11 <= 9.0 ? infoGain: 0.03347107438016533\n",
      "                Left:featureIndex X12 <= 5.0 ? infoGain: 0.04500000000000008\n",
      "                                Left:1.0\n",
      "                                Right:0.0\n",
      "                Right:0.0\n",
      "        Right:featureIndex X14 <= 54.0 ? infoGain: 0.1422222222222222\n",
      "                Left:featureIndex X4 <= 2.0 ? infoGain: 0.1736111111111111\n",
      "                                Left:featureIndex X0 <= 1.0 ? infoGain: 0.21875\n",
      "                                                                Left:1.0\n",
      "                                                                Right:0.0\n",
      "                                Right:1.0\n",
      "                Right:1.0\n",
      " Right:1.0\n"
     ]
    }
   ],
   "source": [
    "#print decision tree\n",
    "dtree.printTree()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b80aff2a19cf941dbb44256e5b2c5db4bea0d59041b979103d19485f1a4c9597"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
